{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "datafactorynewtest"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senarionew"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "flatten"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "splitmultiplerow"
						},
						{
							"name": "splitfunction"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Claim_Number as integer,",
						"          Claimant_Name as string,",
						"          Claim_Type as string,",
						"          Location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> flatten",
						"splitfunction foldDown(unroll(Claim_Type),",
						"     mapColumn(",
						"          Claim_Type,",
						"          Claim_Number,",
						"          Claimant_Name,",
						"          Location",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> splitmultiplerow",
						"flatten derive(Claim_Type = split(Claim_Type, ',')) ~> splitfunction",
						"splitmultiplerow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflowSKkey')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as short,",
						"          name as string,",
						"          gender as string,",
						"          country as string,",
						"          department as short",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 keyGenerate(output(SKEY as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 select(mapColumn(",
						"          id,",
						"          name,",
						"          gender,",
						"          country,",
						"          department,",
						"          SKEY",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_unpivot')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "unpivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CLAIM_ID as integer,",
						"          BUCKET1 as integer,",
						"          BUCKET2 as integer,",
						"          BUCKET3 as integer,",
						"          BUCKET4 as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 unpivot(output(",
						"          BUCKET_VALUE as string,",
						"          BUCKET_AMOUNT as integer",
						"     ),",
						"     ungroupBy(CLAIM_ID),",
						"     lateral: true,",
						"     ignoreNullPivots: false) ~> unpivot1",
						"unpivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dfSplitSinglerow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senarionew"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          skill_list as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 derive(skill_list = split(skill_list, ',')) ~> derivedColumn1",
						"derivedColumn1 foldDown(unroll(skill_list),",
						"     mapColumn(",
						"          empid,",
						"          skill_list",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     partitionFileNames:['tgt_splitrow.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_CummulativeRunningtotalusingDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senarionew"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as short,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 window(asc(salary, true),",
						"     cum_total = sum(salary)) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     partitionFileNames:['emp1tgt.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_MergMultiplerowintoasinglerow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senarionew"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          empname as string,",
						"          skill as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(groupBy(empid),",
						"     skill_list = collect(skill)) ~> aggregate1",
						"aggregate1 derive(skill_list = replace(replace(replace(toString(skill_list),'[',''),']',''),'\"','')) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     partitionFileNames:['tgt_listagg.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_removeduplicate')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senarionew"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "union1"
						},
						{
							"name": "toremoveduprow"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          {name country} as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          empid as integer,",
						"          {name country} as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source2",
						"source1, source2 union(byName: true)~> union1",
						"union1 aggregate(groupBy(empid),",
						"     each(match(name!='empid'), $$ = last($$))) ~> toremoveduprow",
						"toremoveduprow sort(asc(empid, true)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_scdtype1_new')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "New_2025"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "claimfile"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "lkpontgt"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "insertclaim"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "updsink"
						}
					],
					"transformations": [
						{
							"name": "lkpinsertorupd"
						},
						{
							"name": "renamecolumnnamebasedonincommingstream"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "selectrequiredcolumn"
						},
						{
							"name": "splitinsertupdate"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as integer,",
						"          Insurance_type as string,",
						"          insurance_number as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> claimfile",
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as string,",
						"          Insurance_type as string,",
						"          insurance_number as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> lkpontgt",
						"claimfile, lkpontgt lookup(claimfile@Claim_number == lkpontgt@Claim_number,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lkpinsertorupd",
						"lkpinsertorupd select(mapColumn(",
						"          Claim_number = claimfile@Claim_number,",
						"          incident_number = claimfile@incident_number,",
						"          claimant_name = claimfile@claimant_name,",
						"          claimant_address = claimfile@claimant_address,",
						"          payment_type = claimfile@payment_type,",
						"          claim_amount = claimfile@claim_amount,",
						"          Insurance_type = claimfile@Insurance_type,",
						"          insurance_number = claimfile@insurance_number,",
						"          lkp_Claim_number = lkpontgt@Claim_number,",
						"          lkp_incident_number = lkpontgt@incident_number,",
						"          lkp_claimant_name = lkpontgt@claimant_name,",
						"          lkp_claimant_address = lkpontgt@claimant_address,",
						"          lkp_payment_type = lkpontgt@payment_type,",
						"          lkp_claim_amount = lkpontgt@claim_amount,",
						"          lkp_Insurance_type = lkpontgt@Insurance_type,",
						"          lkp_insurance_number = lkpontgt@insurance_number",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamecolumnnamebasedonincommingstream",
						"renamecolumnnamebasedonincommingstream derive(insert_update_logic = iif(isNull(lkp_Claim_number),'i','u')) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          Claim_number,",
						"          incident_number,",
						"          claimant_name,",
						"          claimant_address,",
						"          payment_type,",
						"          claim_amount,",
						"          Insurance_type,",
						"          insurance_number,",
						"          lkp_Claim_number,",
						"          insert_update_logic",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectrequiredcolumn",
						"selectrequiredcolumn split(insert_update_logic=='i',",
						"     insert_update_logic=='u',",
						"     disjoint: false) ~> splitinsertupdate@(insert, update, default)",
						"splitinsertupdate@update alterRow(updateIf(insert_update_logic=='u')) ~> alterRow1",
						"splitinsertupdate@insert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertclaim",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Claim_number'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> updsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_scdtype2_new')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "New_2025"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "lkptgt"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "insTgt"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "updtgt"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						},
						{
							"name": "insertUpdateLogic"
						},
						{
							"name": "split1"
						},
						{
							"name": "select1"
						},
						{
							"name": "derivedColumnActiveflaglogic"
						},
						{
							"name": "derivedColumnActivflagNlogic"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "surrogateKeygenerate"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as string,",
						"          Insurance_type as string,",
						"          insurance_number as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as string,",
						"          Insurance_type as string,",
						"          insurance_number as string,",
						"          sk_key as integer,",
						"          Active_flag as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select * from claim_tgt_scd2data where Active_flag=\\'Y\\'',",
						"     format: 'query') ~> lkptgt",
						"source1, lkptgt lookup(source1@Claim_number == lkptgt@Claim_number,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 derive(v_instr_ubdate_logic = iif(isNull(lkptgt@Claim_number),'i',iif(! isNull(lkptgt@Claim_number) && :src_md5!=:tgt_md5,'u','')),",
						"          src_md5 := md5(source1@incident_number,source1@claimant_name,source1@claimant_address,source1@payment_type,source1@claim_amount,source1@Insurance_type,source1@insurance_number),",
						"          tgt_md5 := md5(lkptgt@incident_number,lkptgt@claimant_name,lkptgt@claimant_address,lkptgt@payment_type,lkptgt@claim_amount,lkptgt@Insurance_type,lkptgt@insurance_number)) ~> insertUpdateLogic",
						"select1 split(v_instr_ubdate_logic=='i'||v_instr_ubdate_logic=='u',",
						"     v_instr_ubdate_logic=='u',",
						"     disjoint: false) ~> split1@(insert, update, default)",
						"insertUpdateLogic select(mapColumn(",
						"          Claim_number = source1@Claim_number,",
						"          incident_number = source1@incident_number,",
						"          claimant_name = source1@claimant_name,",
						"          claimant_address = source1@claimant_address,",
						"          payment_type = source1@payment_type,",
						"          claim_amount = source1@claim_amount,",
						"          Insurance_type = source1@Insurance_type,",
						"          insurance_number = source1@insurance_number,",
						"          Claim_number = lkptgt@Claim_number,",
						"          sk_key,",
						"          v_instr_ubdate_logic",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"surrogateKeygenerate derive(ins_activeflag = 'Y') ~> derivedColumnActiveflaglogic",
						"split1@update derive(upd_active_flag = 'N') ~> derivedColumnActivflagNlogic",
						"derivedColumnActivflagNlogic alterRow(updateIf(1==1)) ~> alterRow1",
						"split1@insert keyGenerate(output(sk_key_generate as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKeygenerate",
						"derivedColumnActiveflaglogic sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          Claim_number,",
						"          incident_number,",
						"          claimant_name,",
						"          claimant_address,",
						"          payment_type,",
						"          claim_amount,",
						"          Insurance_type,",
						"          insurance_number,",
						"          sk_key = sk_key_generate,",
						"          Active_flag = ins_activeflag",
						"     )) ~> insTgt",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['sk_key'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          sk_key,",
						"          Active_flag = upd_active_flag",
						"     )) ~> updtgt"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dfscd2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_azblob_2025",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "lkptgt"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sinkupdate"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						},
						{
							"name": "insertupdatelogic"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "split1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "activeflagNderivedColumn2"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as integer,",
						"          Insurance_type as string,",
						"          insurance_number as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          Claim_number as string,",
						"          incident_number as string,",
						"          claimant_name as string,",
						"          claimant_address as string,",
						"          payment_type as string,",
						"          claim_amount as string,",
						"          Insurance_type as string,",
						"          insurance_number as string,",
						"          sk_key as integer,",
						"          Active_flag as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> lkptgt",
						"source1, lkptgt lookup(source1@Claim_number == lkptgt@Claim_number,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 derive(insert_update_flag = iif(isNull(lkptgt@Claim_number),'i',\r",
						" iif(not(isNull(lkptgt@Claim_number)) && :md5_src != :md5_tgt,'u','')),",
						"          md5_src := md5(source1@incident_number,source1@claimant_name,source1@claimant_address,source1@payment_type,\r",
						"source1@claim_amount,source1@Insurance_type,source1@insurance_number),",
						"          md5_tgt := md5(lkptgt@incident_number,lkptgt@claimant_name,lkptgt@claimant_address,lkptgt@payment_type,\r",
						"lkptgt@claim_amount,lkptgt@Insurance_type,lkptgt@insurance_number)) ~> insertupdatelogic",
						"surrogateKey1 derive(Active_flag = 'Y') ~> derivedColumn1",
						"insertupdatelogic split(insert_update_flag == 'i' ||insert_update_flag == 'u',",
						"     insert_update_flag == 'u',",
						"     disjoint: false) ~> split1@(insert, update)",
						"split1@update alterRow(updateIf(1==1)) ~> alterRow1",
						"alterRow1 derive(Active_flag = 'n') ~> activeflagNderivedColumn2",
						"select2 keyGenerate(output(surrogatekeygenerated as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"split1@insert select(mapColumn(",
						"          Claim_number = split1@insert@Claim_number,",
						"          incident_number = split1@insert@incident_number,",
						"          claimant_name = split1@insert@claimant_name,",
						"          claimant_address = split1@insert@claimant_address,",
						"          payment_type = split1@insert@payment_type,",
						"          claim_amount = split1@insert@claim_amount,",
						"          Insurance_type = split1@insert@Insurance_type,",
						"          insurance_number = split1@insert@insurance_number,",
						"          sk_key,",
						"          Active_flag,",
						"          insert_update_flag",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"activeflagNderivedColumn2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          sk_key,",
						"          Active_flag",
						"     )) ~> sinkupdate",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          Claim_number,",
						"          incident_number,",
						"          claimant_name,",
						"          claimant_address,",
						"          payment_type,",
						"          claim_amount,",
						"          Insurance_type,",
						"          insurance_number,",
						"          sk_key = surrogatekeygenerated,",
						"          Active_flag",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/emp1copy_cumsal_senario3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as short,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 window(asc(salary, true),",
						"     cumsalary = sum(salary)) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     recreate:true,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/scd1flowtabletotable')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "empsrc"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sinkforlkp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						},
						{
							"name": "select1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "split1"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as integer,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> empsrc",
						"source(output(",
						"          emp_id as integer,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> sinkforlkp",
						"empsrc, sinkforlkp lookup(empsrc@emp_id == sinkforlkp@emp_id,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 select(mapColumn(",
						"          emp_id = empsrc@emp_id,",
						"          name = empsrc@name,",
						"          salary = empsrc@salary,",
						"          sink_emp_id = sinkforlkp@emp_id,",
						"          sink_name = sinkforlkp@name,",
						"          sink_salary = sinkforlkp@salary",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 derive(insert_update_logic = iif(isNull(sink_emp_id),'i','u')) ~> derivedColumn1",
						"derivedColumn1 split(insert_update_logic=='i',",
						"     insert_update_logic=='u',",
						"     disjoint: false) ~> split1@(insert, update, default)",
						"split1@update alterRow(updateIf(1==1)) ~> alterRow1",
						"split1@insert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['emp_id'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/scdtype2withlkp')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "empsrc"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sinkforlkp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "lkptocomparesinkandsrc"
						},
						{
							"name": "selecttochangecolumnname"
						},
						{
							"name": "derivedinsertupdateflag"
						},
						{
							"name": "splitinsertupdate"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "select1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as integer,",
						"          e_name as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table',",
						"     partitionBy('hash', 1)) ~> empsrc",
						"source(output(",
						"          sg_key as integer,",
						"          emp_id as integer,",
						"          e_name as string,",
						"          salary as string,",
						"          flag_column as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select * from emp2 where flag_column=1',",
						"     format: 'query') ~> sinkforlkp",
						"empsrc, sinkforlkp lookup(empsrc@emp_id == sinkforlkp@emp_id,",
						"     multiple: true,",
						"     partitionBy('hash', 1),",
						"     broadcast: 'auto',",
						"     pickup: 'any')~> lkptocomparesinkandsrc",
						"lkptocomparesinkandsrc select(mapColumn(",
						"          emp_id = empsrc@emp_id,",
						"          e_name = empsrc@e_name,",
						"          salary = empsrc@salary,",
						"          sg_key,",
						"          sink_emp_id = sinkforlkp@emp_id,",
						"          sink_e_name = sinkforlkp@e_name,",
						"          sink_salary = sinkforlkp@salary,",
						"          flag_column",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selecttochangecolumnname",
						"selecttochangecolumnname derive(insert_update_logic = iif(isNull(sink_emp_id),'i',iif(not(isNull(sink_emp_id)) && :md5_src!=:md5_sink,'u','')),",
						"          md5_src := md5(e_name+salary),",
						"          md5_sink := md5(sink_e_name+sink_salary),",
						"     partitionBy('hash', 1)) ~> derivedinsertupdateflag",
						"select2 split(insert_update_logic=='i' || insert_update_logic=='u',",
						"     insert_update_logic=='u',",
						"     disjoint: false,",
						"     partitionBy('hash', 1)) ~> splitinsertupdate@(insert, update, default)",
						"splitinsertupdate@update alterRow(updateIf(1==1),",
						"     partitionBy('hash', 1)) ~> alterRow1",
						"splitinsertupdate@insert derive(flag_column = 1,",
						"     partitionBy('hash', 1)) ~> derivedColumn1",
						"alterRow1 derive(flag_column = 0,",
						"     partitionBy('hash', 1)) ~> derivedColumn2",
						"splitinsertupdate@update select(mapColumn(",
						"          sg_key,",
						"          insert_update_logic",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"derivedinsertupdateflag select(mapColumn(",
						"          emp_id,",
						"          e_name,",
						"          salary,",
						"          sg_key,",
						"          flag_column,",
						"          insert_update_logic",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          emp_id,",
						"          e_name,",
						"          flag_column",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1",
						"derivedColumn2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['sg_key'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          sg_key = flag_column,",
						"          flag_column",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/senario10_split_singl_col_into_multiple_rows')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as short,",
						"          name as string,",
						"          projects as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 derive(projects = split(projects,',')) ~> derivedColumn1",
						"derivedColumn1 foldDown(unroll(projects),",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          projects",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          emp_id as string,",
						"          name as string,",
						"          salary as string,",
						"          loc as string,",
						"          { email} as string",
						"     ),",
						"     partitionFileNames:['senario10.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/senario11_mrg_multpl_row_val_into_sngl_row')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregatetogroupbyonidandname"
						},
						{
							"name": "derivedColumntoconvertarytostringvalue"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as short,",
						"          name as string,",
						"          projects as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(groupBy(id,",
						"          name),",
						"     projects = collect(projects)) ~> aggregatetogroupbyonidandname",
						"aggregatetogroupbyonidandname derive(projects = replace(replace(replace(toString(projects),'[',''),']',''),'\"','')) ~> derivedColumntoconvertarytostringvalue",
						"derivedColumntoconvertarytostringvalue sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          emp_id as string,",
						"          name as string,",
						"          salary as string,",
						"          loc as string,",
						"          { email} as string",
						"     ),",
						"     partitionFileNames:['senario11tgt_csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/senario5_unique_record_load')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as short,",
						"          name as string,",
						"          gender as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(groupBy(id),",
						"     each(match(name!='id'), $$ = first($$))) ~> aggregate1",
						"aggregate1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/senario6_scd1emp')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "lkpsink"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sinkinsert"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sinkupd"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						},
						{
							"name": "select1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "split1"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as short,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          emp_id as integer,",
						"          name as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> lkpsink",
						"sourceemp, lkpsink lookup(sourceemp@emp_id == lkpsink@emp_id,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 select(mapColumn(",
						"          emp_id = sourceemp@emp_id,",
						"          name = sourceemp@name,",
						"          salary = sourceemp@salary,",
						"          lkp_emp_id = lkpsink@emp_id",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 derive(insert_upd_flag = iif(isNull(lkp_emp_id),'i','u'),",
						"     partitionBy('hash', 1)) ~> derivedColumn1",
						"derivedColumn1 split(insert_upd_flag=='i',",
						"     insert_upd_flag=='u',",
						"     disjoint: false) ~> split1@(insert, update, default)",
						"split1@update alterRow(updateIf(1==1)) ~> alterRow1",
						"split1@insert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     partitionBy('hash', 1)) ~> sinkinsert",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['emp_id'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sinkupd"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/senario7_scdtype2lkp')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "senario"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataset_fordataflow",
								"type": "DatasetReference"
							},
							"name": "emp1src"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sinklkp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sinkinsert"
						},
						{
							"dataset": {
								"referenceName": "AzureSql_dataset_withparam",
								"type": "DatasetReference"
							},
							"name": "sinkupdate"
						}
					],
					"transformations": [
						{
							"name": "lkptocomparesrctgt"
						},
						{
							"name": "selecttochangecolumnname"
						},
						{
							"name": "derivedinsertupdate"
						},
						{
							"name": "derivedColumntoinserttheflagvalueas1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "derivedColumntoupdatetheflagvalue"
						},
						{
							"name": "select1"
						},
						{
							"name": "filter2updatetheflagvalueas0"
						},
						{
							"name": "filter2passinsertnewandupdatedrecord"
						}
					],
					"scriptLines": [
						"source(output(",
						"          emp_id as short,",
						"          name as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp1src",
						"source(output(",
						"          sk_key as integer,",
						"          emp_id as integer,",
						"          name as string,",
						"          salary as string,",
						"          flag as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select * from emp1_tgt where flag=1',",
						"     format: 'query') ~> sinklkp",
						"emp1src, sinklkp lookup(emp1src@emp_id == sinklkp@emp_id,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lkptocomparesrctgt",
						"lkptocomparesrctgt select(mapColumn(",
						"          emp_id = emp1src@emp_id,",
						"          name = emp1src@name,",
						"          salary = emp1src@salary,",
						"          sk_key,",
						"          sink_emp_id = sinklkp@emp_id,",
						"          sink_name = sinklkp@name,",
						"          sink_salary = sinklkp@salary,",
						"          flag",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selecttochangecolumnname",
						"selecttochangecolumnname derive(insert_update_logic = iif(isNull(sink_emp_id), 'i', iif(!isNull(sink_emp_id) && name!=sink_name || salary != sink_salary, 'u', ''))) ~> derivedinsertupdate",
						"filter2passinsertnewandupdatedrecord derive(flag = 1) ~> derivedColumntoinserttheflagvalueas1",
						"filter2updatetheflagvalueas0 alterRow(updateIf(1==1)) ~> alterRow1",
						"alterRow1 derive(flag = 0) ~> derivedColumntoupdatetheflagvalue",
						"derivedinsertupdate select(mapColumn(",
						"          emp_id,",
						"          name,",
						"          salary,",
						"          sk_key,",
						"          flag,",
						"          insert_update_logic",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 filter(insert_update_logic=='u') ~> filter2updatetheflagvalueas0",
						"select1 filter(insert_update_logic=='i'||insert_update_logic=='u') ~> filter2passinsertnewandupdatedrecord",
						"derivedColumntoinserttheflagvalueas1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          emp_id,",
						"          name,",
						"          salary,",
						"          flag",
						"     )) ~> sinkinsert",
						"derivedColumntoupdatetheflagvalue sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['sk_key'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          sk_key,",
						"          flag",
						"     )) ~> sinkupdate"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/skkeyincrimen')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "tgtmaxempidskkey"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ls_azuresql_for_dataflow",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "join1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_name as string,",
						"          emp_sal as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source(output(",
						"          MAX_VALUE as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select coalesce(max(employee_id),0) as MAX_VALUE from dbo.employee_sequence_TGT\\n',",
						"     format: 'query') ~> tgtmaxempidskkey",
						"source1 keyGenerate(output(EMPLOYEE_ID as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1, tgtmaxempidskkey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 derive(MAX_VALUE = MAX_VALUE+EMPLOYEE_ID) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          employee_name,",
						"          emp_sal,",
						"          EMPLOYEE_ID = MAX_VALUE",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Get Metadata_practic')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata_practic",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "azureblob_storage_with_param",
								"type": "DatasetReference",
								"parameters": {
									"contanier_name": "project-folder",
									"filename": {
										"value": "*.csv",
										"type": "Expression"
									},
									"folder_name": "src_folder"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata_practic",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata_practic').output.childItems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "copy data to sqldb",
									"type": "Copy",
									"state": "Inactive",
									"onInactiveMarkAs": "Succeeded",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "AzureBlobStorageReadSettings",
												"recursive": true,
												"wildcardFileName": {
													"value": "@item().name",
													"type": "Expression"
												},
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "SqlServerSink",
											"writeBehavior": "insert",
											"sqlWriterUseTableLock": false,
											"tableOption": "autoCreate"
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "azureblob_storage_with_param",
											"type": "DatasetReference",
											"parameters": {
												"contanier_name": "input",
												"filename": {
													"value": " ",
													"type": "Expression"
												},
												"folder_name": "new"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "sqlserver_db_onprem",
											"type": "DatasetReference",
											"parameters": {
												"schema_name": "dbo",
												"table_name": {
													"value": "@replace(item().name,'.csv','')",
													"type": "Expression"
												}
											}
										}
									]
								},
								{
									"name": "archive the srcfileto arcive  pathand zip",
									"type": "Copy",
									"state": "Inactive",
									"onInactiveMarkAs": "Succeeded",
									"dependsOn": [
										{
											"activity": "copy data to sqldb",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "AzureBlobStorageReadSettings",
												"recursive": true,
												"wildcardFileName": {
													"value": "@item().name",
													"type": "Expression"
												},
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobStorageWriteSettings",
												"copyBehavior": "PreserveHierarchy"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".txt"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "azureblob_storage_with_param",
											"type": "DatasetReference",
											"parameters": {
												"contanier_name": "input",
												"filename": {
													"value": " ",
													"type": "Expression"
												},
												"folder_name": "new"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "dataset_for_arcive",
											"type": "DatasetReference",
											"parameters": {
												"container": "arcive",
												"file": {
													"value": " ",
													"type": "Expression"
												}
											}
										}
									]
								},
								{
									"name": "delete from src after archive",
									"type": "Delete",
									"state": "Inactive",
									"onInactiveMarkAs": "Succeeded",
									"dependsOn": [
										{
											"activity": "archive the srcfileto arcive  pathand zip",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "azureblob_storage_with_param",
											"type": "DatasetReference",
											"parameters": {
												"contanier_name": "input",
												"filename": {
													"value": " ",
													"type": "Expression"
												},
												"folder_name": "new"
											}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobStorageReadSettings",
											"recursive": true,
											"wildcardFileName": {
												"value": "@item().name",
												"type": "Expression"
											},
											"enablePartitionDiscovery": false
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"other_file": {
						"type": "String"
					},
					"employee": {
						"type": "String"
					}
				},
				"folder": {
					"name": "class_HW_folder"
				},
				"annotations": [],
				"lastPublishTime": "2024-12-05T17:42:14Z"
			},
			"dependsOn": []
		}
	]
}